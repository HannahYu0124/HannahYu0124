#I usually run the program at Colab. If you're interested, please check the original version here!
#https://colab.research.google.com/drive/195siefBgf9HDORcJ3705DJc39DbOOypi?usp=sharing


#<<gather csv>>

import pandas as pd
import os

# すべての CSV ファイルが同じフォルダーにあると仮定 (すべての _a をフォルダーに置いてください)
folder_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/果実/5.21'

# 最初の range の最初の数字は最初のサンプル番号に変更 (例えば 1 の場合は 1 と記入、11 から始まる場合は 11 と記入)；2 番目の数字は最後のサンプル数 + 1 に変更 (例えば 6 つの場合は 7 に変更)
# 2 番目の range は 3 つの方向 + 1 に変更
numbering = []
for i in range(1, 89):
    for j in range(1, 4):
        numbering.append(f'{i}-{j}')

# インデックスリストをソート
sorted_numbering = sorted(numbering, key=lambda x: (str(x.split('-')[0]), int(x.split('-')[1])))

# フォルダー内のすべての CSV ファイルを取得
csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]

# すべての列データを保存するための辞書を作成
data_dict = {}

# 各 CSV ファイルを順番に読み込み、規則に従って新しい DataFrame にマージ
for index in sorted_numbering:
    for file_name in csv_files:
        if index in file_name:
            file_path = os.path.join(folder_path, file_name)
            # CSV ファイルを読み込み
            df = pd.read_csv(file_path)
            # 2 列目以降のデータを抽出し、新しい DataFrame の列として設定、列名はファイル名
            absorbance_column = df.iloc[:, 1]
            data_dict[index] = absorbance_column
            break

# すべてのデータを 1 つの DataFrame にマージ
merged_df = pd.DataFrame(data_dict)

# Wavelength (nm) 列を挿入
merged_df.insert(0, 'Wavelength (nm)', df.iloc[:, 0])

# マージした DataFrame を新しい CSV ファイルに書き込む
new_file_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/果実/5.21/肉1-44.csv'
merged_df.to_csv(new_file_path, index=False)
new_file_path_1 = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/果実/5.21/肉1-44.xlsx'
merged_df.to_excel(new_file_path_1, index=False)

merged_df


import pandas as pd
import os

# すべての CSV ファイルが同じフォルダーにあると仮定
folder_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/果実/5.21'

# 最初の range の最初の数字は最初のサンプル番号に変更 (例えば 1 の場合は 1 と記入、11 から始まる場合は 11 と記入)；2 番目の数字は最後のサンプル数 + 1 に変更 (例えば 44 個の場合は 45 に変更)
numbering = list(range(1, 89))

# インデックスリストをソート
sorted_numbering = sorted(numbering)

# フォルダー内のすべての CSV ファイルを取得
csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]

# すべての列データを保存するための辞書を作成
data_dict = {}

# 各 CSV ファイルを順番に読み込み、規則に従って新しい DataFrame にマージ
for number in sorted_numbering:
    for file_name in csv_files:
        if f'{number}' in file_name:
            file_path = os.path.join(folder_path, file_name)
            # CSV ファイルを読み込み
            df = pd.read_csv(file_path)
            # 2 列目以降のデータを抽出し、新しい DataFrame の列として設定、列名はファイル名
            absorbance_column = df.iloc[:, 1]
            data_dict[number] = absorbance_column
            break

# すべてのデータを 1 つの DataFrame にマージ
merged_df = pd.DataFrame(data_dict)

# Wavelength (nm) 列を挿入
merged_df.insert(0, 'Wavelength (nm)', df.iloc[:, 0])

# マージした DataFrame を新しい CSV ファイルに書き込む
new_file_path_csv = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/果実/5.21/肉1-88.csv'
merged_df.to_csv(new_file_path_csv, index=False)

new_file_path_excel = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/果実/5.21/肉1-88.xlsx'
merged_df.to_excel(new_file_path_excel, index=False)

# マージした DataFrame を表示
merged_df


#<<Big csv>>

# pandas ライブラリのインポート
import pandas as pd

# Excel ファイルを読み込み
excel_file_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/糖/肉1-88.xlsx'
df = pd.read_excel(excel_file_path)

# 最初の列を選択
first_column = df.iloc[:, 0]

# 列名に -1、-2、-3 が含まれる列を選択
columns_with_minus = df.filter(regex='-1|-2|-3', axis=1)

# 新しい DataFrame を作成、最初の列と条件に一致する列を含む
new_df = pd.concat([first_column, columns_with_minus], axis=1)

# CSV ファイルとして保存
csv_output_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/糖/肉1-44.csv'
new_df.to_csv(csv_output_path, header=False, index=False)

# データを転置して CSV ファイルとして保存
new_df.T.to_csv(csv_output_path, header=False, index=False)

# DataFrame を表示
new_df


# pandas ライブラリのインポート
import pandas as pd

# Excel ファイルを読み込み
excel_file_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/糖/肉1-88.xlsx'
df = pd.read_excel(excel_file_path)

# 最初の列を選択
first_column = df.iloc[:, 0]

# 列名に数字を含む列を選択
# 列名が文字列で、数字のみの形式（"1"、"2"、"3" など）を含むと仮定
columns_with_numbers = df.filter(regex='^[0-9]+$', axis=1)

# 新しい DataFrame を作成、最初の列と条件に一致する列を含む
new_df = pd.concat([first_column, columns_with_numbers], axis=1)

# CSV ファイルとして保存
csv_output_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/糖/肉1-88.csv'
new_df.to_csv(csv_output_path, header=False, index=False)

# データを転置して CSV ファイルとして保存
new_df.T.to_csv(csv_output_path, header=False, index=False)

# DataFrame を表示
new_df


#<<MSC>>

from sys import stdout
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.csv')#, index_col = 0
df
X = df.values[:,1:]
X

# Define MSC correction function
def msc(input_data):

    # Calculate reference spectrum (e.g., average spectrum)
    reference_spectrum = np.mean(input_data, axis=0)

    # Define a new array and populate it with the corrected data
    output_data = np.zeros_like(input_data)
    for i in range(input_data.shape[0]):

        # Fit a linear model between reference spectrum and individual spectra using least squares method
        coeffs = np.linalg.lstsq(reference_spectrum[:, np.newaxis], input_data[i, :][:, np.newaxis], rcond=None)[0]

        # Compute the corrected spectrum using the linear model coefficients
        output_data[i,:] = input_data[i,:] / (reference_spectrum * coeffs[0])

    return output_data

# Apply MSC correction
Xmsc = msc(X)

# Create a new DataFrame with corrected data
new=df.columns[1:]
test=pd.DataFrame(Xmsc, columns=new)

# Add the additional factor column (e.g., 'Acidity') to the new DataFrame

#new_data=pd.Series(df.loc[:,'Acidity'])
#test.insert(0, 'Acidity', new_data)

#new_data=pd.Series(df.loc[:,'Brix/acid ratio'])
#test.insert(0, 'Brix/acid ratio', new_data)

new_data=pd.Series(df.loc[:,'Brix'])
test.insert(0, 'Brix', new_data)

# Save the new DataFrame to CSV
test.to_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_MSC.csv", index=False)


#<<SNV>>

from sys import stdout
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.csv')#, index_col = 0
df
X = df.values[:,1:]
X

#df1 = pd.read_csv('shell-SVG1.csv')#, index_col = 0
#X1 = df1.values[:,2:]
#wl = np.arange(900,1650,3)
#df2 = pd.read_csv('shell-SVG2.csv')#, index_col = 0
#X2 = df2.values[:,11:261]
#wl = np.arange(900,1650,3)

def snv(input_data):

    # Define a new array and populate it with the corrected data
    output_data = np.zeros_like(input_data)
    for i in range(input_data.shape[0]):

        # Apply correction
        output_data[i,:] = (input_data[i,:] - np.mean(input_data[i,:])) / np.std(input_data[i,:])

    return output_data

# Apply correction
Xsnv = snv(X) # Take the first element of the output tuple
Xsnv

# SNV処理をした新しいデータフレームを作成する
new=df.columns[1:]
test=pd.DataFrame(Xsnv, columns=new)
test

#new_data=pd.Series(df.loc[:,'Acidity'])
#new_data=pd.Series(df.loc[:,'Brix/acid ratio'])
new_data=pd.Series(df.loc[:,'Brix'])

test.insert(0, 'Brix', new_data)
test

test.to_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_SNV.csv", index=False)


#<<SG>>

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sys
from scipy.signal import savgol_filter

x= pd.read_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.csv")
ax =x.iloc[:,1:]
df = np.array(ax)

#改deriv那邊(0, 1, 2)
data1=[]
for n in range(len(x)):
    n=0+n
    svx = savgol_filter(df[n], 5, 2, deriv=2)
    data1.append(svx)
    n=n+1
index=x.index
columns = x.columns[1:]
test= pd.DataFrame(data1,index=index, columns=columns)

bx = x.iloc[:,0]

cx=bx.rename(index='Acidity')

fx=pd.concat([cx,test], axis=1)
fx.to_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_SG2.csv", index=False)


#<<PLS>>

# 酸度

import pandas as pd # pandas のインポート
import numpy as np

# ファイルの場所を変更!
dataset= pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/酸/表11-38.csv') # データセットの読み込み # CSV ソースを変更

# dataset=df.sample(frac=0.1,random_state=99)
dataset.shape  # データセットのサンプル数と特徴量の数を確認
dataset # 念のため確認

# 波長を変更 (分割可能) !!!
x1 = dataset.iloc[:, 1:23]
x2 = dataset.iloc[:, 53:301]
x = pd.concat([x1, x2], axis=1)

# 元のもの
# x = dataset.iloc[:,1:] # スペクトルの特徴量を説明変数 x とする。およそ770nm〜の波長を抽出
y = dataset.iloc[:,0] # APIを目的変数 y とします

from sklearn.model_selection import train_test_split # ランダムにトレーニングデータとテストデータに分割
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True, random_state=99)
autoscaled_x_train = (x_train - x_train.mean()) / x_train.std() # トレーニングデータの説明変数の標準化。平均を引いてから、標準偏差で割ります
autoscaled_x_test = (x_test - x_train.mean()) / x_train.std() # テストデータの説明変数の標準化には、トレーニングデータの平均と標準偏差を用いることに注意してください
autoscaled_y_train = (y_train - y_train.mean()) / y_train.std() # トレーニングデータの目的変数の標準化
from sklearn.model_selection import cross_val_predict # クロスバリデーションにより推定値を計算するために使用
from sklearn.cross_decomposition import PLSRegression # PLS モデル構築やモデルを用いた y の値の推定に使用
from sklearn import metrics # r2 などの指標の計算に使用

max_number_of_components = 20  # 主成分の数
fold_number = 10 # クロスバリデーションのfold数

components = [] # 空のリストを作成し、主成分数をこの変数に追加
r2_in_cv_all = [] # 空のリストを作成し、主成分数ごとのクロスバリデーション後の r2 をこのリストに追加

for component in range(1, max_number_of_components+1):
    model = PLSRegression(n_components=component) # PLS モデルの宣言
    estimated_y_in_cv = pd.DataFrame(cross_val_predict(model, autoscaled_x_train, autoscaled_y_train, cv=fold_number)) # クロスバリデーション推定値の計算し、DataFrame型に変換
    estimated_y_in_cv = estimated_y_in_cv * y_train.std() + y_train.mean() # スケールをもとに戻す
    r2_in_cv = metrics.r2_score(y_train, estimated_y_in_cv) # r2 を計算
    print(component, r2_in_cv) # 主成分数と r2 を表示
    r2_in_cv_all.append(r2_in_cv)  # r2 を追加
    components.append(component) # 主成分数を追加

r2_in_cv_all # 念のため確認
import matplotlib.pyplot as plt # 図の描画に使用
plt.rcParams['font.size'] = 18 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.scatter(components, r2_in_cv_all) # 散布図
plt.xlabel('number of components') # x 軸の名前
plt.ylabel('cross-validated r2') # y 軸の名前
plt.show() # 以上の設定で描画
max(r2_in_cv_all)
r2_in_cv_all.index(max(r2_in_cv_all)) # クロスバリデーション後の r2 が最大値となる index 番号
optimal_component_number = components[r2_in_cv_all.index(max(r2_in_cv_all))] # 最良の主成分数を optimal_component_number に代入
optimal_component_number # 念のため確認
model = PLSRegression(n_components=optimal_component_number) # model = PLSRegression(20)
model.fit(autoscaled_x_train, autoscaled_y_train) # 回帰モデルの構築

standard_regression_coefficients = pd.DataFrame(np.transpose(model.coef_)) # pandas の DataFrame 型に変換
standard_regression_coefficients.columns = x_train.columns # 変数に対応する名前を、元のデータの変数名に
standard_regression_coefficients.index = ['standard_regression_coefficients'] # 列名を変更
standard_regression_coefficients =standard_regression_coefficients.T
standard_regression_coefficients

# SRCのCSV作成
#standard_regression_coefficients.to_csv('/content/drive/MyDrive/京大/colab/文件檔/HSI/酸/HY/HY_SRC_full_d.csv')
standard_regression_coefficients.sort_values(by='standard_regression_coefficients', ascending=False)

wavelength_S=x.columns[0]
wavelength_E=x.columns[-1]

estimated_y_train = pd.DataFrame(model.predict(autoscaled_x_train)) # 推定して、pandas の DataFrame 型に変換
estimated_y_train = estimated_y_train * y_train.std() + y_train.mean() # スケールをもとに戻します
estimated_y_train.index = x_train.index # サンプル名を、元のデータのサンプル名に
estimated_y_train.columns = ['estimated_y'] # 列名を変更
estimated_y_train # 念のため確認 #estimated_y_train.to_csv('estimated_y_train_pls.csv') # CSV ファイルに保存。同じ名前のファイルがあるときは上書きされますので注意してください
import matplotlib.pyplot as plt
import matplotlib.figure as figure # 図の調整に使用
plt.rcParams['font.size'] = 16.5 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.figure(figsize=figure.figaspect(1)) # 図の形を正方形に
plt.scatter(y_train, estimated_y_train.iloc[:, 0]) # 散布図。estimated_y_train は 200×1 の行列のため、0 列目を選択する必要があります
# y_max = max(y_train.max(), estimated_y_train.iloc[:, 0].max()) # 実測値の最大値と推定値の最大値の中で、より大きい値を取得
# y_min = min(y_train.min(), estimated_y_train.iloc[:, 0].min()) # 実測値の最小値と推定値の最小値の中で、より小さい値を取得
plt.scatter(y_train, estimated_y_train.iloc[:, 0], color="cornflowerblue")

# 対角線の数値変更 (0, 1) のみ変更
plt.plot([0, 1], [0, 1], 'k-')
plt.yticks(np.arange(0, 1.1, 0.3))
plt.ylim(0, 1)
plt.xticks(np.arange(0, 1.1, 0.3))
plt.xlim(0, 1)

# 数値の変更 (グラフの刻度の数字)
plt.xticks([0, 0.3, 0.6, 0.9])
plt.yticks([0, 0.3, 0.6, 0.9])

# 名前の変更
plt.xlabel("actual acidity")
plt.ylabel("estimated acidity")
plt.title('training set')
plt.show()

from sklearn import metrics
metrics.r2_score(y_train, estimated_y_train) # r2
metrics.mean_squared_error(y_train, estimated_y_train, squared=False) # RMSE
metrics.mean_absolute_error(y_train, estimated_y_train) # MAE
estimated_y_test = pd.DataFrame(model.predict(autoscaled_x_test)) # 推定して、pandas の DataFrame 型に変換
estimated_y_test = estimated_y_test * y_train.std() + y_train.mean() # スケールをもとに戻します
estimated_y_test.index = x_test.index # サンプル名を、元のデータのサンプル名に
estimated_y_test.columns = ['estimated_y'] # 列名を変更
estimated_y_test # 念のため確認
# estimated_y_test.to_csv('estimated_y_test_pls.csv') # CSV ファイルに保存。同じ名前のファイルがあるときは上書きされますので注意してください
plt.rcParams['font.size'] = 16.5 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.figure(figsize=figure.figaspect(1)) # 図の形を正方形に
plt.scatter(y_test, estimated_y_test.iloc[:, 0]) # 散布図。estimated_y_train は 200×1 の行列のため、0 列目を選択する必要があります
# y_max = max(y_train.max(), estimated_y_train.iloc[:, 0].max()) # 実測値の最大値と推定値の最大値の中で、より大きい値を取得
# y_min = min(y_train.min(), estimated_y_train.iloc[:, 0].min()) # 実測値の最小値と推定値の最小値の中で、より小さい値を取得
plt.scatter(y_test, estimated_y_test.iloc[:, 0], color="cornflowerblue")

# 対角線の数値変更 (0, 1) のみ変更
plt.plot([0, 1], [0, 1], 'k-')
plt.yticks(np.arange(0, 1.1, 0.3))
plt.ylim(0, 1)
plt.xticks(np.arange(0, 1.1, 0.3))
plt.xlim(0, 1)

# 数値の変更 (グラフの刻度の数字)
plt.xticks([0, 0.3, 0.6, 0.9])
plt.yticks([0, 0.3, 0.6, 0.9])

# 名前の変更
plt.xlabel("actual acidity") # x 軸の名前
plt.ylabel("estimated acidity") # y 軸の名前
plt.title('test set')
plt.show()

metrics.r2_score(y_test, estimated_y_test) # r2
metrics.mean_squared_error(y_test, estimated_y_test, squared=False) # RMSE
metrics.mean_absolute_error(y_test, estimated_y_test) # MAE
plt.rcParams['font.size'] = 16.5 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.figure(figsize=figure.figaspect(1)) # 図の形を正方形に
plt.scatter(y_train, estimated_y_train.iloc[:, 0], marker = "o")
plt.scatter(y_test, estimated_y_test.iloc[:, 0], marker = "v")

# 対角線の数値変更 (0, 1) のみ変更
plt.plot([0,1],[0,1], 'k-')
plt.yticks(np.arange(0, 1.1, 0.3))
plt.ylim(0, 1) # y 軸の範囲の設定
plt.xticks(np.arange(0, 1.1, 0.3))
plt.xlim(0, 1) # x 軸の範囲の設定

# 名前の変更
plt.xlabel("actual acidity") # x 軸の名前
plt.ylabel("estimated acidity") # y 軸の名前
plt.legend(["train","test"], bbox_to_anchor=(1.03, 1), loc='upper left', borderaxespad=0, fontsize=14)


#<<PLS (sperated wavelength)>>

import pandas as pd
from sklearn.model_selection import cross_val_predict, train_test_split
from sklearn.cross_decomposition import PLSRegression
from sklearn import metrics

dataset = pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_SG2.csv')

y = dataset.iloc[:, 0]

r2value_train = []
r2value_test = []
MAE_train = []
MAE_test = []
RMSEC = []
RMSEP = []
RPDC = []
RPDP = []
LV = []

#229=1700, 212=1649
regions = [(1, 53),(1, 107),(1, 165),(1, 229),(53, 107),(53, 165),(53, 229),(107, 165),(107, 229),(165, 229)]

for region in regions:
    x = pd.DataFrame()
    for i in range(0, len(region), 2):
        start_col = region[i]
        end_col = region[i + 1]
        x = pd.concat([x, dataset.iloc[:, start_col:end_col]], axis=1)

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True, random_state=99)

    autoscaled_x_train = (x_train - x_train.mean()) / x_train.std()
    autoscaled_x_test = (x_test - x_train.mean()) / x_train.std()
    autoscaled_y_train = (y_train - y_train.mean()) / y_train.std()

    max_number_of_components = 20
    fold_number = 10
    components = []
    r2_in_cv_all = []
    for component in range(1, max_number_of_components + 1):
        model = PLSRegression(n_components=component)
        estimated_y_in_cv = pd.DataFrame(cross_val_predict(model, autoscaled_x_train, autoscaled_y_train, cv=fold_number))
        estimated_y_in_cv = estimated_y_in_cv * y_train.std() + y_train.mean()
        r2_in_cv = metrics.r2_score(y_train, estimated_y_in_cv)
        r2_in_cv_all.append(r2_in_cv)
        components.append(component)

    optimal_component_number = components[r2_in_cv_all.index(max(r2_in_cv_all))]
    LV.append(optimal_component_number)
    model = PLSRegression(n_components=optimal_component_number)
    model.fit(autoscaled_x_train, autoscaled_y_train)

    estimated_y_train = pd.DataFrame(model.predict(autoscaled_x_train))
    estimated_y_train = estimated_y_train * y_train.std() + y_train.mean()

    estimated_y_test = pd.DataFrame(model.predict(autoscaled_x_test))
    estimated_y_test = estimated_y_test * y_train.std() + y_train.mean()

    r2_train = metrics.r2_score(y_train, estimated_y_train)
    r2value_train.append(r2_train)
    mae_train = metrics.mean_absolute_error(y_train, estimated_y_train)
    MAE_train.append(mae_train)
    rmsec = metrics.mean_squared_error(y_train, estimated_y_train, squared=False)
    RMSEC.append(rmsec)
    rpdc = y_train.std() / rmsec
    RPDC.append(rpdc)

    r2_test = metrics.r2_score(y_test, estimated_y_test)
    r2value_test.append(r2_test)
    mae_test = metrics.mean_absolute_error(y_test, estimated_y_test)
    MAE_test.append(mae_test)
    rmsep = metrics.mean_squared_error(y_test, estimated_y_test, squared=False)
    RMSEP.append(rmsep)
    rpdp = y_test.std() / rmsep
    RPDP.append(rpdp)

r2value_train = pd.DataFrame(r2value_train)
MAE_train = pd.DataFrame(MAE_train)
RMSEC = pd.DataFrame(RMSEC)
RPDC = pd.DataFrame(RPDC)
r2value_test = pd.DataFrame(r2value_test)
MAE_test = pd.DataFrame(MAE_test)
RMSEP = pd.DataFrame(RMSEP)
RPDP = pd.DataFrame(RPDP)
LV = pd.DataFrame(LV)

index = []

# 各セグメントを識別するためのインデックスを作成
for region in regions:
    indexx = ""
    for i in range(0, len(region), 2):
        start_wavelength = dataset.columns[region[i]]
        end_wavelength = dataset.columns[region[i + 1] - 1]
        indexx += start_wavelength + '~' + end_wavelength + ", "
    index.append(indexx[:-2])  # 最後のカンマとスペースを削除

data = pd.concat([r2value_train, MAE_train, RMSEC, RPDC, r2value_test, MAE_test, RMSEP, RPDP, LV], axis=1)
data.columns = ['R2_train', 'MAE_train', 'RMSEC', 'RPDC', 'R2_test', 'MAE_test', 'RMSEP', 'RPDP', 'LV']
data.index = index

# 結果をCSVファイルとして保存
output_csv_path = '/content/drive/MyDrive/京大/colab/ファイル/NIRS/test/糖/分段PLS/肉1-88_SG2.csv'
data.to_csv(output_csv_path)
data
