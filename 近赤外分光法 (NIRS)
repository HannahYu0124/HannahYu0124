#I usually run the program at Colab. If you're interested, please check the original version here!
#https://colab.research.google.com/drive/195siefBgf9HDORcJ3705DJc39DbOOypi?usp=sharing


#<<gather csv>>

import pandas as pd
import os

# 假设所有 CSV 文件都在同一个文件夹中 (把_a的都放到資料夾中)
folder_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/果實/5.21'

# 第一個range的第一個數字改成第一個樣本號碼(譬如1就寫1，但如果從11開始就打11)；第二個數字改成最後的樣品數+1 (譬如有6個，就改成7)
# 第二個range改成3個方向+1
numbering = []
for i in range(1, 89):
    for j in range(1, 4):
        numbering.append(f'{i}-{j}')

# 将索引列表排序
sorted_numbering = sorted(numbering, key=lambda x: (str(x.split('-')[0]), int(x.split('-')[1])))

# 获取文件夹中所有 CSV 文件
csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]

# 创建一个字典来存储所有的列数据
data_dict = {}

# 逐个读取每个 CSV 文件，按照规则合并到新 DataFrame 中
for index in sorted_numbering:
    for file_name in csv_files:
        if index in file_name:
            file_path = os.path.join(folder_path, file_name)
            # 读取 CSV 文件
            df = pd.read_csv(file_path)
            # 提取第二列以后的数据，并设置为新 DataFrame 的一列，列名为文件名
            absorbance_column = df.iloc[:, 1]
            data_dict[index] = absorbance_column
            break

# 将所有数据合并到一个 DataFrame 中
merged_df = pd.DataFrame(data_dict)

# 插入 Wavelength (nm) 列
merged_df.insert(0, 'Wavelength (nm)', df.iloc[:, 0])

# 将合并后的 DataFrame 写入新的 CSV 文件
new_file_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/果實/5.21/肉1-44.csv'
merged_df.to_csv(new_file_path, index=False)
new_file_path_1 = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/果實/5.21/肉1-44.csv'
merged_df.to_excel(new_file_path_1, index=False)

merged_df



#單個數字 沒有1-1等 之這個數字

import pandas as pd
import os

# 假设所有 CSV 文件都在同一个文件夹中
folder_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/果實/5.21'

# 第一個range的第一個數字改成第一個樣本號碼(譬如1就寫1，但如果從11開始就打11)；第二個數字改成最後的樣品數+1 (譬如有44個，就改成45)
numbering = list(range(1, 89))

# 将索引列表排序
sorted_numbering = sorted(numbering)

# 获取文件夹中所有 CSV 文件
csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]

# 创建一个字典来存储所有的列数据
data_dict = {}

# 逐个读取每个 CSV 文件，按照规则合并到新 DataFrame 中
for number in sorted_numbering:
    for file_name in csv_files:
        if f'{number}' in file_name:
            file_path = os.path.join(folder_path, file_name)
            # 读取 CSV 文件
            df = pd.read_csv(file_path)
            # 提取第二列以后的数据，并设置为新 DataFrame 的一列，列名为文件名
            absorbance_column = df.iloc[:, 1]
            data_dict[number] = absorbance_column
            break

# 将所有数据合并到一个 DataFrame 中
merged_df = pd.DataFrame(data_dict)

# 插入 Wavelength (nm) 列
merged_df.insert(0, 'Wavelength (nm)', df.iloc[:, 0])

# 将合并后的 DataFrame 写入新的 CSV 文件
new_file_path_csv = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/果實/5.21/肉1-88.csv'
merged_df.to_csv(new_file_path_csv, index=False)

new_file_path_excel = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/果實/5.21/肉1-88.xlsx'
merged_df.to_excel(new_file_path_excel, index=False)

# 顯示合併後的 DataFrame
merged_df


#<<Big csv>>

# 导入pandas库
import pandas as pd

# 读取 Excel 文件
excel_file_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.xlsx'
df = pd.read_excel(excel_file_path)

# 选择第一列
first_column = df.iloc[:, 0]

# 选择列名中包含-1、-2、-3 的列
columns_with_minus = df.filter(regex='-1|-2|-3', axis=1)

# 创建新的DataFrame，包括第一列和符合条件的列
new_df = pd.concat([first_column, columns_with_minus], axis=1)

# 保存为 CSV 文件
csv_output_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-44.csv'
new_df.to_csv(csv_output_path, header=False, index=False)

# 转置数据并保存为 CSV 文件
new_df.T.to_csv(csv_output_path, header=False, index=False)

# 打印 DataFrame
new_df


#單個數字 沒有1-1等 之這個數字

# 导入pandas库
import pandas as pd

# 读取 Excel 文件
excel_file_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.xlsx'
df = pd.read_excel(excel_file_path)

# 选择第一列
first_column = df.iloc[:, 0]

# 选择列名中包含數字的列
# 假设列名是字符串，包含數字的形式如 "1", "2", "3" 等
columns_with_numbers = df.filter(regex='^[0-9]+$', axis=1)

# 创建新的DataFrame，包括第一列和符合条件的列
new_df = pd.concat([first_column, columns_with_numbers], axis=1)

# 保存为 CSV 文件
csv_output_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.csv'
new_df.to_csv(csv_output_path, header=False, index=False)

# 转置数据并保存为 CSV 文件
new_df.T.to_csv(csv_output_path, header=False, index=False)

# 打印 DataFrame
new_df


#<<MSC>>

from sys import stdout
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.csv')#, index_col = 0
df
X = df.values[:,1:]
X

# Define MSC correction function
def msc(input_data):

    # Calculate reference spectrum (e.g., average spectrum)
    reference_spectrum = np.mean(input_data, axis=0)

    # Define a new array and populate it with the corrected data
    output_data = np.zeros_like(input_data)
    for i in range(input_data.shape[0]):

        # Fit a linear model between reference spectrum and individual spectra using least squares method
        coeffs = np.linalg.lstsq(reference_spectrum[:, np.newaxis], input_data[i, :][:, np.newaxis], rcond=None)[0]

        # Compute the corrected spectrum using the linear model coefficients
        output_data[i,:] = input_data[i,:] / (reference_spectrum * coeffs[0])

    return output_data

# Apply MSC correction
Xmsc = msc(X)

# Create a new DataFrame with corrected data
new=df.columns[1:]
test=pd.DataFrame(Xmsc, columns=new)

# Add the additional factor column (e.g., 'Acidity') to the new DataFrame

#new_data=pd.Series(df.loc[:,'Acidity'])
#test.insert(0, 'Acidity', new_data)

#new_data=pd.Series(df.loc[:,'Brix/acid ratio'])
#test.insert(0, 'Brix/acid ratio', new_data)

new_data=pd.Series(df.loc[:,'Brix'])
test.insert(0, 'Brix', new_data)

# Save the new DataFrame to CSV
test.to_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_MSC.csv", index=False)


#<<SNV>>

from sys import stdout
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.csv')#, index_col = 0
df
X = df.values[:,1:]
X

#df1 = pd.read_csv('shell-SVG1.csv')#, index_col = 0
#X1 = df1.values[:,2:]
#wl = np.arange(900,1650,3)
#df2 = pd.read_csv('shell-SVG2.csv')#, index_col = 0
#X2 = df2.values[:,11:261]
#wl = np.arange(900,1650,3)

def snv(input_data):

    # Define a new array and populate it with the corrected data
    output_data = np.zeros_like(input_data)
    for i in range(input_data.shape[0]):

        # Apply correction
        output_data[i,:] = (input_data[i,:] - np.mean(input_data[i,:])) / np.std(input_data[i,:])

    return output_data

# Apply correction
Xsnv = snv(X) # Take the first element of the output tuple
Xsnv

# SNV処理をした新しいデータフレームを作成する
new=df.columns[1:]
test=pd.DataFrame(Xsnv, columns=new)
test

#改
#new_data=pd.Series(df.loc[:,'Acidity'])
#new_data=pd.Series(df.loc[:,'Brix/acid ratio'])
new_data=pd.Series(df.loc[:,'Brix'])

#改
test.insert(0, 'Brix', new_data)
test

#這邊記得改
test.to_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_SNV.csv", index=False)


#<<SG>>

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sys
from scipy.signal import savgol_filter

x= pd.read_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88.csv")
ax =x.iloc[:,1:]
df = np.array(ax)

#改deriv那邊(0, 1, 2)
data1=[]
for n in range(len(x)):
    n=0+n
    svx = savgol_filter(df[n], 5, 2, deriv=2)
    data1.append(svx)
    n=n+1
index=x.index
columns = x.columns[1:]
test= pd.DataFrame(data1,index=index, columns=columns)

bx = x.iloc[:,0]

cx=bx.rename(index='Acidity')

fx=pd.concat([cx,test], axis=1)
fx.to_csv("/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_SG2.csv", index=False)


#<<PLS>>

#酸度

import pandas as pd # pandas のインポート
import numpy as np

#改檔案位置!
dataset= pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/表11-38.csv') # データセットの読み込み #改csv來源

#dataset=df.sample(frac=0.1,random_state=99)
dataset.shape  # データセットのサンプル数、特徴量の数の確認
dataset # 念のため確認

#改波長 (可分段)!!!
x1 = dataset.iloc[:, 1:23]
x2 = dataset.iloc[:, 53:301]
x = pd.concat([x1, x2], axis=1)

#原本的
#x = dataset.iloc[:,1:] # スペクトルの特徴量を説明変数 x とする。およそ770nm〜の波長を抽出
y = dataset.iloc[:,0] # APIを目的変数 y とします

from sklearn.model_selection import train_test_split # ランダムにトレーニングデータとテストデータとに分割。
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True, random_state=99)
autoscaled_x_train = (x_train - x_train.mean()) / x_train.std() # トレーニングデータの説明変数の標準化。平均を引いてから、標準偏差で割ります
autoscaled_x_test = (x_test - x_train.mean()) / x_train.std() # テストデータの説明変数の標準化には、トレーニングデータの平均と標準偏差を用いることに注意してください
autoscaled_y_train = (y_train - y_train.mean()) / y_train.std() # トレーニングデータの目的変数の標準化
from sklearn.model_selection import cross_val_predict # クロスバリデーションにより推定値を計算するために使用
from sklearn.cross_decomposition import PLSRegression # PLS モデル構築やモデルを用いた y の値の推定に使用
from sklearn import metrics # r2 等の指標の計算に使用

max_number_of_components = 20  # 主成分の数
fold_number = 10 # クロスバリデーションのfold数

components = [] # 空の list を作成して、主成分数をこの変数に追加
r2_in_cv_all = [] # 空の list を作成して、主成分数ごとのクロスバリデーション後の r2 をこの list に追加

for component in range(1, max_number_of_components+1):
    model = PLSRegression(n_components=component) # PLS モデルの宣言
    estimated_y_in_cv = pd.DataFrame(cross_val_predict(model, autoscaled_x_train, autoscaled_y_train, cv=fold_number)) # クロスバリデーション推定値の計算し、DataFrame型に変換
    estimated_y_in_cv = estimated_y_in_cv * y_train.std() + y_train.mean() # スケールをもとに戻す
    r2_in_cv = metrics.r2_score(y_train, estimated_y_in_cv) # r2 を計算
    print(component, r2_in_cv) # 主成分数と r2 を表示
    r2_in_cv_all.append(r2_in_cv)  # r2 を追加
    components.append(component) # 主成分数を追加

r2_in_cv_all # 念のため確認
import matplotlib.pyplot as plt # 図の描画に使用
plt.rcParams['font.size'] = 18 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.scatter(components, r2_in_cv_all) # 散布図
plt.xlabel('number of components') # x 軸の名前
plt.ylabel('cross-validated r2') # y 軸の名前
plt.show() # 以上の設定で描画
max(r2_in_cv_all)
r2_in_cv_all.index(max(r2_in_cv_all)) # クロスバリデーション後の r2 が最大値となる index 番号
optimal_component_number = components[r2_in_cv_all.index(max(r2_in_cv_all))] # 最良の主成分数を optimal_component_number に代入
optimal_component_number # 念のため確認
model = PLSRegression(n_components=optimal_component_number) # model = PLSRegression(20)
model.fit(autoscaled_x_train, autoscaled_y_train) # 回帰モデルの構築

standard_regression_coefficients = pd.DataFrame(np.transpose(model.coef_)) # pandas の DataFrame 型に変換
standard_regression_coefficients.columns = x_train.columns # 変数に対応する名前を、元のデータの変数名に
standard_regression_coefficients.index = ['standard_regression_coefficients'] # 列名を変更
standard_regression_coefficients =standard_regression_coefficients.T
standard_regression_coefficients

#SRC的csv製作
#standard_regression_coefficients.to_csv ('/content/drive/MyDrive/京大/colab/文件檔/HSI/酸/HY/HY_SRC_full_d.csv')
standard_regression_coefficients.sort_values(by='standard_regression_coefficients', ascending=False)

wavelength_S=x.columns[0]
wavelength_E=x.columns[-1]

estimated_y_train = pd.DataFrame(model.predict(autoscaled_x_train)) # 推定して、pandas の DataFrame 型に変換
estimated_y_train = estimated_y_train * y_train.std() + y_train.mean() # スケールをもとに戻します
estimated_y_train.index = x_train.index # サンプル名を、元のデータのサンプル名に
estimated_y_train.columns = ['estimated_y'] # 列名を変更
estimated_y_train # 念のため確認 #estimated_y_train.to_csv('estimated_y_train_pls.csv') # csv ファイルに保存。同じ名前のファイルがあるときは上書きされますので注意してください
import matplotlib.pyplot as plt
import matplotlib.figure as figure # 図の調整に使用
plt.rcParams['font.size'] = 16.5 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.figure(figsize=figure.figaspect(1)) # 図の形を正方形に
plt.scatter(y_train, estimated_y_train.iloc[:, 0]) # 散布図。estimated_y_train は 200×1 の行列のため、0 列目を選択する必要があります
#y_max = max(y_train.max(), estimated_y_train.iloc[:, 0].max()) # 実測値の最大値と、推定値の最大値の中で、より大きい値を取得
#y_min = min(y_train.min(), estimated_y_train.iloc[:, 0].min()) # 実測値の最小値と、推定値の最小値の中で、より小さい値を取得
plt.scatter(y_train, estimated_y_train.iloc[:, 0], color="cornflowerblue")

#更改對角線數值 改(0, 1)那幾個就好
plt.plot([0, 1], [0, 1], 'k-')
plt.yticks(np.arange(0, 1.1, 0.3))
plt.ylim(0, 1)
plt.xticks(np.arange(0, 1.1, 0.3))
plt.xlim(0, 1)

#更改數值(圖刻度的數字)
plt.xticks([0, 0.3, 0.6, 0.9])
plt.yticks([0, 0.3, 0.6, 0.9])

#更改名稱
plt.xlabel("actual acidity")
plt.ylabel("estimated acidity")
plt.title('training set')
plt.show()

from sklearn import metrics
metrics.r2_score(y_train, estimated_y_train) # r2
metrics.mean_squared_error(y_train,estimated_y_train,squared=False) #RMSE
metrics.mean_absolute_error(y_train, estimated_y_train) # MAE
estimated_y_test = pd.DataFrame(model.predict(autoscaled_x_test)) # 推定して、pandas の DataFrame 型に変換
estimated_y_test = estimated_y_test * y_train.std() + y_train.mean() # スケールをもとに戻します
estimated_y_test.index = x_test.index # サンプル名を、元のデータのサンプル名に
estimated_y_test.columns = ['estimated_y'] # 列名を変更
estimated_y_test # 念のため確
#estimated_y_test.to_csv('estimated_y_test_pls.csv') # csv ファイルに保存。同じ名前のファイルがあるときは上書きされますので注意してください
plt.rcParams['font.size'] = 16.5 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.figure(figsize=figure.figaspect(1)) # 図の形を正方形に
plt.scatter(y_test, estimated_y_test.iloc[:, 0]) # 散布図。estimated_y_train は 200×1 の行列のため、0 列目を選択する必要があります
#y_max = max(y_train.max(), estimated_y_train.iloc[:, 0].max()) # 実測値の最大値と、推定値の最大値の中で、より大きい値を取得
#y_min = min(y_train.min(), estimated_y_train.iloc[:, 0].min()) # 実測値の最小値と、推定値の最小値の中で、より小さい値を取得
plt.scatter(y_test, estimated_y_test.iloc[:, 0], color="cornflowerblue")

#更改對角線數值 改(0, 1)那幾個就好
plt.plot([0, 1], [0, 1], 'k-')
plt.yticks(np.arange(0, 1.1, 0.3))
plt.ylim(0, 1)
plt.xticks(np.arange(0, 1.1, 0.3))
plt.xlim(0, 1)

#更改數值(圖刻度的數字)
plt.xticks([0, 0.3, 0.6, 0.9])
plt.yticks([0, 0.3, 0.6, 0.9])

#更改名稱
plt.xlabel("actual acidity")
plt.ylabel("estimated acidity")
plt.title('test set')
plt.show()

metrics.r2_score(y_test, estimated_y_test) # r2
metrics.mean_squared_error(y_test,estimated_y_test,squared=False) #RMSE
metrics.mean_absolute_error(y_test, estimated_y_test) # MAE
plt.rcParams['font.size'] = 16.5 # 横軸や縦軸の名前の文字などのフォントのサイズ
plt.figure(figsize=figure.figaspect(1)) # 図の形を正方形に
plt.scatter(y_train, estimated_y_train.iloc[:, 0],marker = "o")
plt.scatter(y_test, estimated_y_test.iloc[:, 0],marker = "v")

#更改對角線數值 除改(0, 1)對角線，(0, 1.1, 0.3) 中間改最大數字，第三個改刻度值
plt.plot([0,1],[0,1], 'k-')
plt.yticks(np.arange(0, 1.1, 0.3))
plt.ylim(0, 1) # y 軸の範囲の設定
plt.xticks(np.arange(0, 1.1, 0.3))
plt.xlim(0, 1) # x 軸の範囲の設定

#更改名稱
plt.xlabel("actual acidity") # x 軸の名前
plt.ylabel("estimated acidity") # y 軸の名前
plt.legend(["train","test"],bbox_to_anchor=(1.03, 1), loc='upper left', borderaxespad=0, fontsize=14)



#<<PLS (sperated wavelength)>>

import pandas as pd
from sklearn.model_selection import cross_val_predict, train_test_split
from sklearn.cross_decomposition import PLSRegression
from sklearn import metrics

dataset = pd.read_csv('/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/肉1-88_SG2.csv')

y = dataset.iloc[:, 0]

r2value_train = []
r2value_test = []
MAE_train = []
MAE_test = []
RMSEC = []
RMSEP = []
RPDC = []
RPDP = []
LV = []

# 定义不同区段的起始列和结束列 注意：确保结束列不超过数据集列的大小
#229=1700, 212=1649
regions = [(1, 53),(1, 107),(1, 165),(1, 229),(53, 107),(53, 165),(53, 229),(107, 165),(107, 229),(165, 229)]

for region in regions:
    x = pd.DataFrame()
    for i in range(0, len(region), 2):
        start_col = region[i]
        end_col = region[i + 1]
        x = pd.concat([x, dataset.iloc[:, start_col:end_col]], axis=1)

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True, random_state=99)

    autoscaled_x_train = (x_train - x_train.mean()) / x_train.std()
    autoscaled_x_test = (x_test - x_train.mean()) / x_train.std()
    autoscaled_y_train = (y_train - y_train.mean()) / y_train.std()

    max_number_of_components = 20
    fold_number = 10
    components = []
    r2_in_cv_all = []
    for component in range(1, max_number_of_components + 1):
        model = PLSRegression(n_components=component)
        estimated_y_in_cv = pd.DataFrame(cross_val_predict(model, autoscaled_x_train, autoscaled_y_train, cv=fold_number))
        estimated_y_in_cv = estimated_y_in_cv * y_train.std() + y_train.mean()
        r2_in_cv = metrics.r2_score(y_train, estimated_y_in_cv)
        r2_in_cv_all.append(r2_in_cv)
        components.append(component)

    optimal_component_number = components[r2_in_cv_all.index(max(r2_in_cv_all))]
    LV.append(optimal_component_number)
    model = PLSRegression(n_components=optimal_component_number)
    model.fit(autoscaled_x_train, autoscaled_y_train)

    estimated_y_train = pd.DataFrame(model.predict(autoscaled_x_train))
    estimated_y_train = estimated_y_train * y_train.std() + y_train.mean()

    estimated_y_test = pd.DataFrame(model.predict(autoscaled_x_test))
    estimated_y_test = estimated_y_test * y_train.std() + y_train.mean()

    r2_train = metrics.r2_score(y_train, estimated_y_train)
    r2value_train.append(r2_train)
    mae_train = metrics.mean_absolute_error(y_train, estimated_y_train)
    MAE_train.append(mae_train)
    rmsec = metrics.mean_squared_error(y_train, estimated_y_train, squared=False)
    RMSEC.append(rmsec)
    rpdc = y_train.std() / rmsec
    RPDC.append(rpdc)

    r2_test = metrics.r2_score(y_test, estimated_y_test)
    r2value_test.append(r2_test)
    mae_test = metrics.mean_absolute_error(y_test, estimated_y_test)
    MAE_test.append(mae_test)
    rmsep = metrics.mean_squared_error(y_test, estimated_y_test, squared=False)
    RMSEP.append(rmsep)
    rpdp = y_test.std() / rmsep
    RPDP.append(rpdp)

r2value_train = pd.DataFrame(r2value_train)
MAE_train = pd.DataFrame(MAE_train)
RMSEC = pd.DataFrame(RMSEC)
RPDC = pd.DataFrame(RPDC)
r2value_test = pd.DataFrame(r2value_test)
MAE_test = pd.DataFrame(MAE_test)
RMSEP = pd.DataFrame(RMSEP)
RPDP = pd.DataFrame(RPDP)
LV = pd.DataFrame(LV)

index = []

# 创建索引以标识每个区段
for region in regions:
    indexx = ""
    for i in range(0, len(region), 2):
        start_wavelength = dataset.columns[region[i]]
        end_wavelength = dataset.columns[region[i + 1] - 1]
        indexx += start_wavelength + '~' + end_wavelength + ", "
    index.append(indexx[:-2])  # 删除最后的逗号和空格

data = pd.concat([r2value_train, MAE_train, RMSEC, RPDC, r2value_test, MAE_test, RMSEP, RPDP, LV], axis=1)
data.columns = ['R2_train', 'MAE_train', 'RMSEC', 'RPDC', 'R2_test', 'MAE_test', 'RMSEP', 'RPDP', 'LV']
data.index = index

# 将结果保存到CSV文件中
output_csv_path = '/content/drive/MyDrive/京大/colab/文件檔/NIRS/test/糖/分段PLS/肉1-88_SG2.csv'
data.to_csv(output_csv_path)
data
